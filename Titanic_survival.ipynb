{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca089417",
   "metadata": {},
   "source": [
    "# Predict Titanic Survival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696fd26",
   "metadata": {},
   "source": [
    "The RMS Titanic set sail on its maiden voyage in 1912, crossing the Atlantic from Southampton, England to New York City. The ship never completed the voyage, sinking to the bottom of the Atlantic Ocean after hitting an iceberg, bringing down 1,502 of 2,224 passengers onboard.\n",
    "In this project you will create a Logistic Regression model that predicts which passengers survived the sinking of the Titanic, based on features like age and class. The data we will be using for training our model is provided by Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95778801",
   "metadata": {},
   "source": [
    "The file passengers.csv contains the data of 892 passengers onboard the Titanic when it sank that fateful day. Let’s begin by loading the data into a pandas DataFrame named passengers. Print passengers and inspect the columns. What features could we use to predict survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e410bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Load the passenger data\n",
    "passengers=pd.read_csv('passengers.csv')\n",
    "print(passengers)\n",
    "print(passengers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0a682",
   "metadata": {},
   "source": [
    "Given the saying, “women and children first,” Sex and Age seem like good features to predict survival. Let’s map the text values in the Sex column to a numerical value. Update Sex such that all values female are replaced with 1 and all values male are replaced with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d337d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "886    0\n",
      "887    1\n",
      "888    1\n",
      "889    0\n",
      "890    0\n",
      "Name: Sex, Length: 891, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "passengers['Sex']=np.where(passengers['Sex']==\"female\",1,0)\n",
    "print(passengers.Sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c0127",
   "metadata": {},
   "source": [
    "Let’s take a look at Age. Print passengers['Age'].values. You can see we have multiple missing values, or nans. Fill all the empty Age values in passengers with the mean age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd97f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.   38.   26.   35.   35.     nan 54.    2.   27.   14.    4.   58.\n",
      " 20.   39.   14.   55.    2.     nan 31.     nan 35.   34.   15.   28.\n",
      "  8.   38.     nan 19.     nan   nan 40.     nan   nan 66.   28.   42.\n",
      "   nan 21.   18.   14.   40.   27.     nan  3.   19.     nan   nan   nan\n",
      "   nan 18.    7.   21.   49.   29.   65.     nan 21.   28.5   5.   11.\n",
      " 22.   38.   45.    4.     nan   nan 29.   19.   17.   26.   32.   16.\n",
      " 21.   26.   32.   25.     nan   nan  0.83 30.   22.   29.     nan 28.\n",
      " 17.   33.   16.     nan 23.   24.   29.   20.   46.   26.   59.     nan\n",
      " 71.   23.   34.   34.   28.     nan 21.   33.   37.   28.   21.     nan\n",
      " 38.     nan 47.   14.5  22.   20.   17.   21.   70.5  29.   24.    2.\n",
      " 21.     nan 32.5  32.5  54.   12.     nan 24.     nan 45.   33.   20.\n",
      " 47.   29.   25.   23.   19.   37.   16.   24.     nan 22.   24.   19.\n",
      " 18.   19.   27.    9.   36.5  42.   51.   22.   55.5  40.5    nan 51.\n",
      " 16.   30.     nan   nan 44.   40.   26.   17.    1.    9.     nan 45.\n",
      "   nan 28.   61.    4.    1.   21.   56.   18.     nan 50.   30.   36.\n",
      "   nan   nan  9.    1.    4.     nan   nan 45.   40.   36.   32.   19.\n",
      " 19.    3.   44.   58.     nan 42.     nan 24.   28.     nan 34.   45.5\n",
      " 18.    2.   32.   26.   16.   40.   24.   35.   22.   30.     nan 31.\n",
      " 27.   42.   32.   30.   16.   27.   51.     nan 38.   22.   19.   20.5\n",
      " 18.     nan 35.   29.   59.    5.   24.     nan 44.    8.   19.   33.\n",
      "   nan   nan 29.   22.   30.   44.   25.   24.   37.   54.     nan 29.\n",
      " 62.   30.   41.   29.     nan 30.   35.   50.     nan  3.   52.   40.\n",
      "   nan 36.   16.   25.   58.   35.     nan 25.   41.   37.     nan 63.\n",
      " 45.     nan  7.   35.   65.   28.   16.   19.     nan 33.   30.   22.\n",
      " 42.   22.   26.   19.   36.   24.   24.     nan 23.5   2.     nan 50.\n",
      "   nan   nan 19.     nan   nan  0.92   nan 17.   30.   30.   24.   18.\n",
      " 26.   28.   43.   26.   24.   54.   31.   40.   22.   27.   30.   22.\n",
      "   nan 36.   61.   36.   31.   16.     nan 45.5  38.   16.     nan   nan\n",
      " 29.   41.   45.   45.    2.   24.   28.   25.   36.   24.   40.     nan\n",
      "  3.   42.   23.     nan 15.   25.     nan 28.   22.   38.     nan   nan\n",
      " 40.   29.   45.   35.     nan 30.   60.     nan   nan 24.   25.   18.\n",
      " 19.   22.    3.     nan 22.   27.   20.   19.   42.    1.   32.   35.\n",
      "   nan 18.    1.   36.     nan 17.   36.   21.   28.   23.   24.   22.\n",
      " 31.   46.   23.   28.   39.   26.   21.   28.   20.   34.   51.    3.\n",
      " 21.     nan   nan   nan 33.     nan 44.     nan 34.   18.   30.   10.\n",
      "   nan 21.   29.   28.   18.     nan 28.   19.     nan 32.   28.     nan\n",
      " 42.   17.   50.   14.   21.   24.   64.   31.   45.   20.   25.   28.\n",
      "   nan  4.   13.   34.    5.   52.   36.     nan 30.   49.     nan 29.\n",
      " 65.     nan 50.     nan 48.   34.   47.   48.     nan 38.     nan 56.\n",
      "   nan  0.75   nan 38.   33.   23.   22.     nan 34.   29.   22.    2.\n",
      "  9.     nan 50.   63.   25.     nan 35.   58.   30.    9.     nan 21.\n",
      " 55.   71.   21.     nan 54.     nan 25.   24.   17.   21.     nan 37.\n",
      " 16.   18.   33.     nan 28.   26.   29.     nan 36.   54.   24.   47.\n",
      " 34.     nan 36.   32.   30.   22.     nan 44.     nan 40.5  50.     nan\n",
      " 39.   23.    2.     nan 17.     nan 30.    7.   45.   30.     nan 22.\n",
      " 36.    9.   11.   32.   50.   64.   19.     nan 33.    8.   17.   27.\n",
      "   nan 22.   22.   62.   48.     nan 39.   36.     nan 40.   28.     nan\n",
      "   nan 24.   19.   29.     nan 32.   62.   53.   36.     nan 16.   19.\n",
      " 34.   39.     nan 32.   25.   39.   54.   36.     nan 18.   47.   60.\n",
      " 22.     nan 35.   52.   47.     nan 37.   36.     nan 49.     nan 49.\n",
      " 24.     nan   nan 44.   35.   36.   30.   27.   22.   40.   39.     nan\n",
      "   nan   nan 35.   24.   34.   26.    4.   26.   27.   42.   20.   21.\n",
      " 21.   61.   57.   21.   26.     nan 80.   51.   32.     nan  9.   28.\n",
      " 32.   31.   41.     nan 20.   24.    2.     nan  0.75 48.   19.   56.\n",
      "   nan 23.     nan 18.   21.     nan 18.   24.     nan 32.   23.   58.\n",
      " 50.   40.   47.   36.   20.   32.   25.     nan 43.     nan 40.   31.\n",
      " 70.   31.     nan 18.   24.5  18.   43.   36.     nan 27.   20.   14.\n",
      " 60.   25.   14.   19.   18.   15.   31.    4.     nan 25.   60.   52.\n",
      " 44.     nan 49.   42.   18.   35.   18.   25.   26.   39.   45.   42.\n",
      " 22.     nan 24.     nan 48.   29.   52.   19.   38.   27.     nan 33.\n",
      "  6.   17.   34.   50.   27.   20.   30.     nan 25.   25.   29.   11.\n",
      "   nan 23.   23.   28.5  48.   35.     nan   nan   nan 36.   21.   24.\n",
      " 31.   70.   16.   30.   19.   31.    4.    6.   33.   23.   48.    0.67\n",
      " 28.   18.   34.   33.     nan 41.   20.   36.   16.   51.     nan 30.5\n",
      "   nan 32.   24.   48.   57.     nan 54.   18.     nan  5.     nan 43.\n",
      " 13.   17.   29.     nan 25.   25.   18.    8.    1.   46.     nan 16.\n",
      "   nan   nan 25.   39.   49.   31.   30.   30.   34.   31.   11.    0.42\n",
      " 27.   31.   39.   18.   39.   33.   26.   39.   35.    6.   30.5    nan\n",
      " 23.   31.   43.   10.   52.   27.   38.   27.    2.     nan   nan  1.\n",
      "   nan 62.   15.    0.83   nan 23.   18.   39.   21.     nan 32.     nan\n",
      " 20.   16.   30.   34.5  17.   42.     nan 35.   28.     nan  4.   74.\n",
      "  9.   16.   44.   18.   45.   51.   24.     nan 41.   21.   48.     nan\n",
      " 24.   42.   27.   31.     nan  4.   26.   47.   33.   47.   28.   15.\n",
      " 20.   19.     nan 56.   25.   33.   22.   28.   25.   39.   27.   19.\n",
      "   nan 26.   32.  ]\n",
      "0      22.000000\n",
      "1      38.000000\n",
      "2      26.000000\n",
      "3      35.000000\n",
      "4      35.000000\n",
      "         ...    \n",
      "886    27.000000\n",
      "887    19.000000\n",
      "888    29.699118\n",
      "889    26.000000\n",
      "890    32.000000\n",
      "Name: Age, Length: 891, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Fill the nan values in the age column\n",
    "print(passengers.Age.values)\n",
    "passengers['Age'].fillna(passengers['Age'].mean(),inplace=True)\n",
    "print(passengers.Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a2825",
   "metadata": {},
   "source": [
    "Given the strict class system onboard the Titanic, let’s utilize the Pclass column, or the passenger class, as another feature. Create a new column named FirstClass that stores 1 for all passengers in first class and 0 for all other passengers.\n",
    "\n",
    "Create a new column named SecondClass that stores 1 for all passengers in second class and 0 for all other passengers.\n",
    "\n",
    "Print passengers and inspect the DataFrame to ensure all the updates have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d17078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name  Sex        Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    0  22.000000      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.000000      1   \n",
      "2                               Heikkinen, Miss. Laina    0  26.000000      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.000000      1   \n",
      "4                             Allen, Mr. William Henry    0  35.000000      0   \n",
      "..                                                 ...  ...        ...    ...   \n",
      "886                              Montvila, Rev. Juozas    0  27.000000      0   \n",
      "887                       Graham, Miss. Margaret Edith    0  19.000000      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"    0  29.699118      1   \n",
      "889                              Behr, Mr. Karl Howell    0  26.000000      0   \n",
      "890                                Dooley, Mr. Patrick    0  32.000000      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  FirstClass  SecondClass  \n",
      "0        0         A/5 21171   7.2500   NaN        S           0            0  \n",
      "1        0          PC 17599  71.2833   C85        C           1            0  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S           0            0  \n",
      "3        0            113803  53.1000  C123        S           1            0  \n",
      "4        0            373450   8.0500   NaN        S           0            0  \n",
      "..     ...               ...      ...   ...      ...         ...          ...  \n",
      "886      0            211536  13.0000   NaN        S           0            1  \n",
      "887      0            112053  30.0000   B42        S           1            0  \n",
      "888      2        W./C. 6607  23.4500   NaN        S           0            0  \n",
      "889      0            111369  30.0000  C148        C           1            0  \n",
      "890      0            370376   7.7500   NaN        Q           0            0  \n",
      "\n",
      "[891 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a first class column\n",
    "passengers['FirstClass']=passengers.Pclass.apply(lambda x: 1 if x==1 else 0)\n",
    "#print(passengers)\n",
    "# Create a second class column\n",
    "passengers['SecondClass']=passengers.Pclass.apply(lambda x: 1 if x==2 else 0)\n",
    "print(passengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5876acdd",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, let’s select the columns we want to build our model on. Select columns Sex, Age, FirstClass, and SecondClass and store them in a variable named features. Select column Survived and store it a variable named survival.\n",
    "\n",
    "Split the data into training and test sets using sklearn‘s train_test_split() method. We’ll use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16305e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "# Select the desired features\n",
    "features=passengers[['Sex','Age','FirstClass','SecondClass']]\n",
    "survival=passengers['Survived']\n",
    "\n",
    "# Perform train, test, split\n",
    "X_train,X_test,y_train,y_test=train_test_split(features,survival,test_size =0.3)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f0324",
   "metadata": {},
   "source": [
    "Since sklearn‘s Logistic Regression implementation uses Regularization, we need to scale our feature data. Create a StandardScaler object, .fit_transform() it on the training features, and .transform() the test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9263c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature data so it has mean = 0 and standard deviation = 1\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7962a7c",
   "metadata": {},
   "source": [
    "Create a LogisticRegression model with sklearn and .fit() it on the training data.\n",
    "\n",
    "Fitting the model will perform gradient descent to find the feature coefficients that minimize the log-loss for the training data.\n",
    "\n",
    "Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc52925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7078651685393258\n",
      "0.667910447761194\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the train data\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "\n",
    "# Score the model on the test data\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06973a37",
   "metadata": {},
   "source": [
    "Print the feature coefficients determined by the model. Which feature is most important in predicting survival on the sinking of the Titanic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9505b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.43218985  0.96944388  0.51756115]]\n",
      "[('Sex', 0.0), ('Age', -0.43218985001281623), ('FirstClass', 0.9694438835782401), ('SecondClass', 0.5175611474036539)]\n"
     ]
    }
   ],
   "source": [
    "# Analyze the coefficients\n",
    "print(model.coef_)\n",
    "print(list(zip(['Sex','Age','FirstClass','SecondClass'],model.coef_[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc620053",
   "metadata": {},
   "source": [
    "Let’s use our model to make predictions on the survival of a few fateful passengers. Provided in the code editor is information for 3rd class passenger Jack and 1st class passenger Rose, stored in NumPy arrays. The arrays store 4 feature values, in the following order:\n",
    "\n",
    "Sex, represented by a 0 for male and 1 for female\n",
    "Age, represented as an integer in years\n",
    "FirstClass, with a 1 indicating the passenger is in first class\n",
    "SecondClass, with a 1 indicating the passenger is in second class\n",
    "A third array, You, is also provided in the code editor with empty feature values. Uncomment the line containing You and update the array with your information, or the information for some fictitious passenger. Make sure to enter all values as floats with a .!\n",
    "\n",
    "Combine Jack, Rose, and You into a single NumPy array named sample_passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6173ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample passenger features\n",
    "Jack = np.array([0.0,20.0,0.0,0.0])\n",
    "Rose = np.array([1.0,17.0,1.0,0.0])\n",
    "You = np.array([0.0,32,1.0,0.0])\n",
    "\n",
    "# Combine passenger arrays\n",
    "sample_passengers=np.array([Jack,Rose,You])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648ebcd",
   "metadata": {},
   "source": [
    "Since our Logistic Regression model was trained on scaled feature data, we must also scale the feature data we are making predictions on. Using the StandardScaler object created earlier, apply its .transform() method to sample_passengers and save the result to sample_passengers.\n",
    "\n",
    "Print sample_passengers to view the scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "346b9681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -2.31464569 -1.82823697 -1.75200703]\n",
      " [ 1.         -2.33230455  3.9116181  -1.75200703]\n",
      " [ 0.         -2.24401028  3.9116181  -1.75200703]]\n"
     ]
    }
   ],
   "source": [
    "# Scale the sample passenger features\n",
    "sample_passengers = scaler.transform(sample_passengers)\n",
    "print(sample_passengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25814dff",
   "metadata": {},
   "source": [
    "Who will survive, and who will sink? Use your model’s .predict() method on sample_passengers and print the result to find out.\n",
    "\n",
    "Want to see the probabilities that led to these predictions? Call your model’s .predict_proba() method on sample_passengers and print the result. The 1st column is the probability of a passenger perishing on the Titanic, and the 2nd column is the probability of a passenger surviving the sinking (which was calculated by our model to make the final classification decision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85994fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90205497 0.09794503]\n",
      " [0.03383528 0.96616472]\n",
      " [0.03510518 0.96489482]]\n"
     ]
    }
   ],
   "source": [
    "# Make survival predictions!\n",
    "model.predict(sample_passengers)\n",
    "print(model.predict_proba(sample_passengers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528160f2",
   "metadata": {},
   "source": [
    "It appears rose and You have a better chance of survival than Jack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16c5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
